Usage

* What I should know 
 
 GFF documents are completely indexed/stored with the provided schema. This means that the original document is not required (and can be deleted, or not available from index server).
 For Genbank sheets, the script genbank2gff3.pl can be used to convert a file to GFF3 format (GFF+Fasta) and indexed as such.
 GFF index integrates the indexed data with GBrowse. Other formats cannot be integrated, only GFF is supported.
 
 For a description of the GFF fields, refer to {{{gff.html}this page}}.
 
 The following fields are added by the indexer CrawlerIndex, and are document dependent.
 Those are used in web interface and to extract documents (lookup.jsp).
 
+-- 
    <!-- Those fields are not searchable, they are stored only for later analysis -->
    <field name="stream_name" type="lowercase" indexed="false" stored="true"/>
    <field name="stream_content_type" type="lowercase" indexed="false" stored="true"/>
+--    

    <stream_name> contains the original file name and path. It will be used by lookup.jsp to download the content from the original document. This means of course that document must be
     accessible with its original path from the index server.
    
    <stream_content_type> contains the content-type of the document. For Biological data it will be <<biosequence/xxx>> (gff, embl,...).
    Others will be usual content-type (text/plain, text/html...)

 <<Important fields>>:
    
+--

  <field name="id" type="lowercase" indexed="true" stored="true"/>
  <field name="file" type="lowercase" indexed="false" stored="true"/>
  
+--

  * id field is the "key" to access a bio element. It may not be unique depending on input documents. It can be the name of a chromosome, or a protein ...
  In the storage backend, it refers to the transcript of the element for GFF entries (if available).
  
  * file field is added by the indexer to specify the location of the element in the original document. It is used by the lookup.jsp file to get start and end positions in the source file.
  
* Web query

 A Web interface is provided to query the index, but it can also be queried directly. Indeed, Solr implements an HTTP interface to query the index.
 To query all shards, one should use the query type <shard> defined in solrconfig.xml help with the parameter  "qt".
 
 For Solr syntax, look at {{{http://wiki.apache.org/solr/CommonQueryParameters} Solr CommonQueryParameters}} and {{{http://wiki.apache.org/solr/SolJSON} JSON output}} .
 
 Example query for search for <protein> in all indexes with a JSON output format (xml is default):

+-- 
 http://localhost:8983/solr/select/?q=protein&qt=shard&wt=json
+--

 The Web GUI is accessible directly at the root url of the application and code is hosted in the webapp directory of the application.

 
* How to index documents

 To index a document (be it composed of several documents such as multi fasta, or gff), one should use:

+--

 For usage: java -jar CrawlerIndex-0.1-jar-with-dependencies.jar -h
 
 Example:
 Index with no storage
 java -jar CrawlerIndex-0.1-jar-with-dependencies.jar -b uniprot -f /tmp/uniprot.dat -t embl 

 Index with storage, clean index for specified bank before
 java -jar CrawlerIndex-0.1-jar-with-dependencies.jar -c -b mypersonalbank -f /tmp/my.gff -t gff -store -stHost http://myhost

+--

* Posting document

 Help with Solr and Tika, it is possible to post via HTTP a document and to index it automatically.
 However only readseq conversion is supported here and it will work only if original document is a unique document (no multi fasta for example).
 To do so, refer to Solr documentation. There is no auto-detect implementation for the moment, so content-type should be specified (biosequence/embl for example).